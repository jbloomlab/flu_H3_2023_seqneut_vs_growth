{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23a54e40-0156-4559-8cd8-7912fa362d5d",
   "metadata": {},
   "source": [
    "# Multiple regression with titers and number of HA1 mutations\n",
    "We know that neutralization titers (specifically, the fraction of individuals with low neutralization titesr, as well as mean and median titers) and number of HA1 mutations both correlate strongly with MLR-estimated strain growth rates. These metrics are also collinear. Using multiple regression, we want to determine which of the predictors (neutralization titers or HA1 mutations) more fully explain the dependent outcome variable (growth rate). \n",
    "\n",
    "Author: Caroline Kikawa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd36643c-1b47-43b0-bd10-17dabf3aae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import Ridge\n",
    "import statsmodels.api\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Input and output directories\n",
    "datadir = '../data'\n",
    "resultsdir = '../results'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1590b7e1-1f8d-4516-b998-9b0fbef5fdb2",
   "metadata": {},
   "source": [
    "Select the `growth_vs_titers` input to use from the top-level `results` directory. Note there are many different model fits to choose from and neutralization titers to choose from. I selected:\n",
    "* MLR growth rate estimates from models fit to HA1 sequences within 1 amino acid mutation of a library strain with minimum 80 sequencing counts\n",
    "* Neutralization titers from children and pre-vaccination adults (95 total individual sera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4ecde851-303e-475c-b5ab-6addce3d3db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "growth_vs_titers = pd.read_csv(os.path.join(resultsdir, 'growth_vs_titers/growth_vs_titers_gisaid-ha1-within1_2023-mincounts80_child-and-adultprevax-sera_scatter.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c78d7e4-6d2d-4efe-ac93-a68976d08a1a",
   "metadata": {},
   "source": [
    "## Repeated train-test splitting (80-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb02caf1-a38e-4338-ab8f-9bf637247470",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = growth_vs_titers\n",
    "X = X[['HA1_protein_mutations', 'frac_below_titer']]  \n",
    "y = growth_vs_titers.growth_advantage_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e7fcc99-dfb9-4253-b558-4b046f8e92b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler = StandardScaler() # Uncomment to use z-scoring instead of min-max scaling\n",
    "\n",
    "# Standardize X values \n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1d94713-65b0-4bcf-840c-95674c67fdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine if X raw data or standardized data should be used\n",
    "X = X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98019798-cca8-45da-a7ec-280b38f8bdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train-test set MSE: 0.0011\n",
      "Standard deviation of MSE: 0.0007\n",
      "\n",
      "Average betas:\n",
      " Intercept                0.986701\n",
      "HA1_protein_mutations    0.035113\n",
      "frac_below_titer         0.047537\n",
      "dtype: float64 \n",
      "\n",
      "Standard deviation of betas:\n",
      " Intercept                0.005296\n",
      "HA1_protein_mutations    0.005174\n",
      "frac_below_titer         0.006979\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "n_splits = 500  # Number of different random splits\n",
    "mse_list = []\n",
    "betas = []  # Store beta coefficients\n",
    "\n",
    "for _ in range(n_splits):\n",
    "    # Randomly split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=None)\n",
    "\n",
    "    # Train and evaluate model\n",
    "    TT_model = LinearRegression()\n",
    "    TT_model.fit(X_train, y_train)\n",
    "    y_pred = TT_model.predict(X_test)\n",
    "\n",
    "    # Store MSE\n",
    "    mse_list.append(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "    # Store beta coefficients (intercept + slopes)\n",
    "    beta_values = np.append(TT_model.intercept_, TT_model.coef_)  # [β0, β1, β2]\n",
    "    betas.append(beta_values)\n",
    "\n",
    "# Convert betas into a DataFrame\n",
    "betas_df = pd.DataFrame(betas, columns=['Intercept'] + list(growth_vs_titers[['HA1_protein_mutations', 'frac_below_titer']].columns))\n",
    "\n",
    "# Compute summary statistics\n",
    "betas_mean = betas_df.mean()\n",
    "betas_std = betas_df.std()\n",
    "\n",
    "print(f\"Average train-test set MSE: {np.mean(mse_list):.4f}\")\n",
    "print(f\"Standard deviation of MSE: {np.std(mse_list):.4f}\\n\")\n",
    "\n",
    "print('Average betas:\\n', betas_mean, '\\n')\n",
    "print('Standard deviation of betas:\\n', betas_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b447f701-2a88-42d9-a27a-ee273f086388",
   "metadata": {},
   "source": [
    "## LeaveOneOut cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71cb8cc5-26c4-42d6-869c-715e48586a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeaveOneOut average MSE: 0.001007555509317768\n",
      "LeaveOneOut standard deviation of MSE: 0.0010380823835710949\n",
      "\n",
      "Intercept average: 0.9866383292270368\n",
      "Beta average for HA1_mutations: 0.03517572563840705\n",
      "Beta average for frac_below_cutoff: 0.047214317537812765\n",
      "\n",
      "Intercept standard deviation: 0.002645016115010733\n",
      "Beta standard deviation for HA1_mutations: 0.002614126027510468\n",
      "Beta standard deviation for frac_below_cutoff: 0.0034241336974872687\n"
     ]
    }
   ],
   "source": [
    "# Initialize LeaveOneOUt module\n",
    "leaveOneOut = LeaveOneOut()\n",
    "# Initialize lists for model values\n",
    "mse_list = []\n",
    "beta_list = []\n",
    "intercept_list = []\n",
    "\n",
    "for train_index, test_index in leaveOneOut.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    loo_model = LinearRegression()\n",
    "    loo_model.fit(X_train, y_train)\n",
    "    y_pred = loo_model.predict(X_test)\n",
    "\n",
    "    mse_list.append(mean_squared_error(y_test, y_pred))\n",
    "    beta_list.append(loo_model.coef_)\n",
    "    intercept_list.append(loo_model.intercept_)\n",
    "    \n",
    "# Calculate average MSE across all splits\n",
    "average_mse = np.mean(mse_list)\n",
    "std_mse = np.std(mse_list, ddof=1) \n",
    "\n",
    "print(\"LeaveOneOut average MSE:\", average_mse)\n",
    "print(\"LeaveOneOut standard deviation of MSE:\", std_mse)\n",
    "\n",
    "# Calculate average and standard deviation of betas\n",
    "mean_X1 = sum(row[0] for row in beta_list) / len(beta_list)\n",
    "mean_X2 = sum(row[1] for row in beta_list) / len(beta_list)\n",
    "\n",
    "std_values = np.std(beta_list, axis=0, ddof=1)  \n",
    "\n",
    "print(\"\\nIntercept average:\",  np.mean(intercept_list))\n",
    "print(\"Beta average for HA1_mutations:\", mean_X1)\n",
    "print(\"Beta average for frac_below_cutoff:\", mean_X2)\n",
    "\n",
    "print(\"\\nIntercept standard deviation:\",  np.std(intercept_list))\n",
    "print(\"Beta standard deviation for HA1_mutations:\", std_values[0])\n",
    "print(\"Beta standard deviation for frac_below_cutoff:\", std_values[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5722de-0364-4c2a-be75-db911145e70c",
   "metadata": {},
   "source": [
    "## Quantify multicollinearity with variance inflation factor (VIF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51bc67bd-9bd5-4aa8-840e-c8a3448061bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float64(2.784140488315549), np.float64(2.7841404883155483)]\n"
     ]
    }
   ],
   "source": [
    "X_scaled_with_intercept = np.column_stack((np.ones(len(X_scaled)), X_scaled))  # Add intercept for VIF calc\n",
    "vif = [variance_inflation_factor(X_scaled_with_intercept, i) for i in range(1, X_scaled_with_intercept.shape[1])]\n",
    "print(vif)  # Ignore the intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f93efc-0cf0-4d7d-a9c1-a992f1761bc1",
   "metadata": {},
   "source": [
    "This isn't a *huge* VIF, meaning the betas are somewhat reliable but definitely still influenced by multicollinearity. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bedba4c-5955-496f-852e-0f075849af3c",
   "metadata": {},
   "source": [
    "## Permutation test (feature importance via shuffling)\n",
    "We can shuffle the predictors and see how model performance is affected. Larger drops in MSE from the shuffling of a given variable implies that that variable is more important.\n",
    "https://scikit-learn.org/stable/modules/permutation_importance.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "74b2df6e-9dc2-4f4d-b668-4fe2603f478c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average HA1 mutations importance: 0.003261011522286963\n",
      "Average fraction titers below cutoff importance: 0.006376129068762077\n",
      "Percentage contribution of HA1 mutations  33.83795734302958\n",
      "Percentage contribution of titers  66.16204265697043\n"
     ]
    }
   ],
   "source": [
    "def permutation_importance(model, X, y, n_permutations=100):\n",
    "    base_mse = mean_squared_error(y, model.predict(X))\n",
    "    importances = []\n",
    "    \n",
    "    for col in range(X.shape[1]):\n",
    "        permuted_mse = []\n",
    "        for _ in range(n_permutations):\n",
    "            X_permuted = X.copy()\n",
    "            X_permuted[:, col] = shuffle(X[:, col], random_state=42)  # Shuffle one column\n",
    "            permuted_mse.append(mean_squared_error(y, model.predict(X_permuted)))\n",
    "        \n",
    "        importance = np.mean(permuted_mse) - base_mse\n",
    "        importances.append(importance)\n",
    "    \n",
    "    return importances\n",
    "\n",
    "# Lists for importance values\n",
    "HA1_importances=[]\n",
    "titers_importances=[]\n",
    "\n",
    "for n in range(100):\n",
    "    # Train a model \n",
    "    # Randomly split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=None)\n",
    "    # Fit the model\n",
    "    TT_model = LinearRegression()\n",
    "    TT_model.fit(X_train, y_train)\n",
    "    \n",
    "    importances = permutation_importance(TT_model, X, y)\n",
    "\n",
    "    HA1_importances.append(importances[0])\n",
    "    titers_importances.append(importances[1])\n",
    "\n",
    "print(f\"Average HA1 mutations importance: {np.mean(HA1_importances)}\")\n",
    "print(f\"Average fraction titers below cutoff importance: {np.mean(titers_importances)}\")\n",
    "\n",
    "# Report relative importance\n",
    "total_importance = np.mean(HA1_importances) + np.mean(titers_importances)\n",
    "HA1_relative_importance = np.mean(HA1_importances) / total_importance * 100\n",
    "titers_relative_importance = np.mean(titers_importances) / total_importance * 100\n",
    "\n",
    "print('Percentage contribution of HA1 mutations ', HA1_relative_importance) \n",
    "print('Percentage contribution of titers ', titers_relative_importance)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04b1ba8-700c-4558-bb0c-04ae6123c822",
   "metadata": {},
   "source": [
    "So by this metric, titers are the primary driver of predictions (>50%) but HA1 mutations are moderately important (20-50%).\n",
    "\n",
    "**What if the data isn't scaled?** The relative importances shouldnt change, since the MSE isn't dependent on variable magnitude. But here I'll quickly test that..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5cff742c-b001-4afc-8ef7-e424aad2734a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average HA1 mutations importance: 0.00312471208145247\n",
      "Average fraction titers below cutoff importance: 0.006604026769537033\n",
      "Percentage contribution of HA1 mutations  32.11836733735183\n",
      "Percentage contribution of titers  67.88163266264817\n"
     ]
    }
   ],
   "source": [
    "# Recalculate using unscaled X\n",
    "X = growth_vs_titers[['HA1_protein_mutations', 'frac_below_titer']].values\n",
    "\n",
    "# Lists for importance values\n",
    "HA1_importances=[]\n",
    "titers_importances=[]\n",
    "\n",
    "for n in range(100):\n",
    "    # Train a model \n",
    "    # Randomly split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=None)\n",
    "    # Fit the model\n",
    "    TT_model = LinearRegression()\n",
    "    TT_model.fit(X_train, y_train)\n",
    "    \n",
    "    importances = permutation_importance(TT_model, X, y)\n",
    "\n",
    "    HA1_importances.append(importances[0])\n",
    "    titers_importances.append(importances[1])\n",
    "\n",
    "print(f\"Average HA1 mutations importance: {np.mean(HA1_importances)}\")\n",
    "print(f\"Average fraction titers below cutoff importance: {np.mean(titers_importances)}\")\n",
    "\n",
    "# Report relative importance\n",
    "total_importance = np.mean(HA1_importances) + np.mean(titers_importances)\n",
    "HA1_relative_importance = np.mean(HA1_importances) / total_importance * 100\n",
    "titers_relative_importance = np.mean(titers_importances) / total_importance * 100\n",
    "\n",
    "print('Percentage contribution of HA1 mutations ', HA1_relative_importance) \n",
    "print('Percentage contribution of titers ', titers_relative_importance) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79a4ac9-dc29-4dba-a7ce-3b682014d33d",
   "metadata": {},
   "source": [
    "These contributions of HA1 mutations and titers are not meaningfully different without scaling, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eaee1b-7ce9-407e-8bd6-8fe37acbcb9f",
   "metadata": {},
   "source": [
    "## Quantifying the amount of variance explained by each predictor\n",
    "By re-fitting model to datasets without different predictor variables, we can estimate how much of the variance is explained by different predictors. More important \"driving\" predictors will explain greater proportion of variance. Here I repurposed a lot of code from Jesse (https://dms-vep.org/SARS-CoV-2_XBB.1.5_spike_DMS/notebooks/current_dms_compare_natural_ba2_ba5_xbb.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1117a8ff-5b93-4572-850c-f4a477b39ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "phenotype_basic_colors = {\n",
    "    \"HA1_protein_mutations\": \"red\",\n",
    "    \"frac_below_titer\": \"blue\",\n",
    "\n",
    "}\n",
    "\n",
    "phenotypes = list(phenotype_basic_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0d89fbcc-a3f9-463f-aa24-1093dab5610a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the scaler\n",
    "# scaler = MinMaxScaler()\n",
    "scaler = StandardScaler() # For smaller datasets, z-scoring is preferable over min-max scaling\n",
    "\n",
    "# Standardize X values \n",
    "phenotypes_scaled = pd.DataFrame(scaler.fit_transform(growth_vs_titers[['HA1_protein_mutations', 'frac_below_titer']]), columns=['HA1_protein_mutations', 'frac_below_titer'])\n",
    "phenotypes_scaled['growth_advantage_median'] = growth_vs_titers.growth_advantage_median.tolist()\n",
    "\n",
    "# Standardize data\n",
    "ols_df = phenotypes_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "383f977c-6145-46ed-a5b8-a9f4d79698e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of points\n",
    "n = len(ols_df)\n",
    "\n",
    "# Number of randomizations\n",
    "n_rand = 200\n",
    "\n",
    "# Randomize data\n",
    "randomized_dfs = []\n",
    "for i in range(n_rand):\n",
    "    randomized_df = ols_df.apply(np.random.permutation).copy()  # Shuffle each column independently\n",
    "    randomized_df['randomization'] = i  # Add identifier column\n",
    "    randomized_dfs.append(randomized_df)\n",
    "\n",
    "# Concatenate all randomized dataframes\n",
    "ols_df_rand = pd.concat(randomized_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2f145c8a-cadf-4572-abdf-0e7edced646f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For randomized DMS data, P < 0.005: 0 of 200 have r >= observed value of 0.954\n",
      "OLS regression r2:  0.9110570127505621\n",
      "Variance explained by each predictor:\n",
      "HA1_protein_mutations: 7% of variance (coef 0.0352 ± 0.01358306)\n",
      "frac_below_titer: 12% of variance (coef 0.0470 ± 0.01358306)\n"
     ]
    }
   ],
   "source": [
    "def ols_unique_var_explained(var_endog, vars, df, full_r2):\n",
    "    \"\"\"Get unique variance explained by fitting model after removing each variable.\n",
    "\n",
    "    https://blog.minitab.com/en/adventures-in-statistics-2/how-to-identify-the-most-important-predictor-variables-in-regression-models\n",
    "    \n",
    "    \"\"\"\n",
    "    unique_var = {}\n",
    "    for vremove in vars:\n",
    "        vremove_ols_model = statsmodels.api.OLS(\n",
    "            endog=df[[var_endog]],\n",
    "            exog=statsmodels.api.add_constant(df[[v for v in vars if v != vremove]].astype(float)),\n",
    "        )\n",
    "        vremove_res_ols = vremove_ols_model.fit()\n",
    "        unique_var[vremove] = full_r2 - vremove_res_ols.rsquared\n",
    "    return unique_var\n",
    "\n",
    "\n",
    "\n",
    "# https://www.einblick.ai/python-code-examples/ordinary-least-squares-regression-statsmodels/\n",
    "ols_model = statsmodels.api.OLS(\n",
    "    endog=ols_df[[\"growth_advantage_median\"]],\n",
    "    exog=statsmodels.api.add_constant(ols_df[phenotypes].astype(float)),\n",
    ")\n",
    "res_ols = ols_model.fit()\n",
    "ols_df = ols_df.assign(predicted_change_in_growth_rate=res_ols.predict())\n",
    "r2 = res_ols.rsquared\n",
    "r = math.sqrt(r2)\n",
    "unique_var = ols_unique_var_explained(\"growth_advantage_median\", phenotypes, ols_df, r2)\n",
    "\n",
    "subtitle = [\n",
    "    # https://stackoverflow.com/a/53966201\n",
    "    f\"{p}: {unique_var[p] * 100:.0f}% of variance (coef {res_ols.params[p]:.4f} \\u00B1 {res_ols.bse[p]:.8f})\"\n",
    "    for p in phenotypes\n",
    "]\n",
    "\n",
    "# Number of points\n",
    "n = len(ols_df)\n",
    "\n",
    "# Randomized fits\n",
    "rand_rs = []\n",
    "for _, ols_df_rand_i in ols_df_rand.groupby(\"randomization\"):\n",
    "    ols_model_rand_i = statsmodels.api.OLS(\n",
    "        endog=ols_df_rand_i[[\"growth_advantage_median\"]],\n",
    "        exog=statsmodels.api.add_constant(ols_df_rand_i[phenotypes].astype(float)),\n",
    "    )\n",
    "    res_ols_rand_i = ols_model_rand_i.fit()\n",
    "    rand_rs.append(math.sqrt(res_ols_rand_i.rsquared))\n",
    "n_ge = sum(rand_r >= r for rand_r in rand_rs)\n",
    "if n_ge:\n",
    "    p_str = f\"P = {n_ge / len(rand_rs)}\"\n",
    "else:\n",
    "    p_str = f\"P < {1 / len(rand_rs)}\" \n",
    "# Print p-values\n",
    "print(f\"For randomized DMS data, {p_str}: {n_ge} of {len(rand_rs)} have r >= observed value of {r:.3f}\")\n",
    "\n",
    "# Print output from model fit and variance contributions\n",
    "print('OLS regression r2: ', r2)\n",
    "print('Variance explained by each predictor:')\n",
    "for item in subtitle: print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a410498f-ce77-408f-80cc-c9639db4b6b7",
   "metadata": {},
   "source": [
    "So frac_below_titer explains more variance, which is consistent with the MSE-drop permutation test I did above. The coefficients are low (because data is standardized and estimated growth rates are 0.9-1.1), and actually basically the same as from the `sklearn` model. \n",
    "\n",
    "Below, we rerun the analysis on unscaled data to show that variance explained by each variable doesn't change whether or not the data is scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "331402fe-f550-419a-8a28-e42a437530d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For randomized DMS data, P < 0.005: 0 of 200 have r >= observed value of 0.954\n",
      "OLS regression r2:  0.9110570127505621\n",
      "Variance explained by each predictor:\n",
      "HA1_protein_mutations: 7% of variance (coef 0.0166 ± 0.00638834)\n",
      "frac_below_titer: 12% of variance (coef 0.7178 ± 0.20725169)\n"
     ]
    }
   ],
   "source": [
    "# Unscaled data\n",
    "unscaled_ols_df = growth_vs_titers[['HA1_protein_mutations', 'frac_below_titer', 'growth_advantage_median']]\n",
    "\n",
    "# https://www.einblick.ai/python-code-examples/ordinary-least-squares-regression-statsmodels/\n",
    "ols_model = statsmodels.api.OLS(\n",
    "    endog=ols_df[[\"growth_advantage_median\"]],\n",
    "    exog=statsmodels.api.add_constant(unscaled_ols_df[phenotypes].astype(float)),\n",
    ")\n",
    "res_ols = ols_model.fit()\n",
    "unscaled_ols_df = unscaled_ols_df.assign(predicted_change_in_growth_rate=res_ols.predict())\n",
    "r2 = res_ols.rsquared\n",
    "r = math.sqrt(r2)\n",
    "unique_var = ols_unique_var_explained(\"growth_advantage_median\", phenotypes, unscaled_ols_df, r2)\n",
    "\n",
    "subtitle = [\n",
    "    # https://stackoverflow.com/a/53966201\n",
    "    f\"{p}: {unique_var[p] * 100:.0f}% of variance (coef {res_ols.params[p]:.4f} \\u00B1 {res_ols.bse[p]:.8f})\"\n",
    "    for p in phenotypes\n",
    "]\n",
    "\n",
    "# Number of points\n",
    "n = len(unscaled_ols_df)\n",
    "\n",
    "# Randomized fits\n",
    "rand_rs = []\n",
    "for _, ols_df_rand_i in ols_df_rand.groupby(\"randomization\"):\n",
    "    ols_model_rand_i = statsmodels.api.OLS(\n",
    "        endog=ols_df_rand_i[[\"growth_advantage_median\"]],\n",
    "        exog=statsmodels.api.add_constant(ols_df_rand_i[phenotypes].astype(float)),\n",
    "    )\n",
    "    res_ols_rand_i = ols_model_rand_i.fit()\n",
    "    rand_rs.append(math.sqrt(res_ols_rand_i.rsquared))\n",
    "n_ge = sum(rand_r >= r for rand_r in rand_rs)\n",
    "if n_ge:\n",
    "    p_str = f\"P = {n_ge / len(rand_rs)}\"\n",
    "else:\n",
    "    p_str = f\"P < {1 / len(rand_rs)}\" \n",
    "# Print p-values\n",
    "print(f\"For randomized DMS data, {p_str}: {n_ge} of {len(rand_rs)} have r >= observed value of {r:.3f}\")\n",
    "\n",
    "# Print output from model fit and variance contributions\n",
    "print('OLS regression r2: ', r2)\n",
    "print('Variance explained by each predictor:')\n",
    "for item in subtitle: print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb98b505-4d97-4a42-9fcd-a2d286a07857",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
